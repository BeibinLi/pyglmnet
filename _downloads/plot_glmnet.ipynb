{
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "execution_count": null,
      "outputs": [],
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Elastic net regularized GLMs\n\n\nTutorial on Elastic net Regularized Generalized Linear Model.\nWe will go through the math and gradient descent algorithm to\nin order to optimize GLM cost function.\n\n**Reference**\nJerome Friedman, Trevor Hastie and Rob Tibshirani. (2010).\nRegularization Paths for Generalized Linear Models via Coordinate Descent.\nJournal of Statistical Software, Vol. 33(1), 1-22 `[pdf]\n<https://core.ac.uk/download/files/153/6287975.pdf>`_.\n\n"
      ]
    },
    {
      "execution_count": null,
      "outputs": [],
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "# Author: Pavan Ramkumar\n# License: MIT\n\nimport numpy as np\nfrom scipy.special import expit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "GLM with elastic net penalty\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nIn Generalized Linear Model (GLM), we mainly want to solve the following problem.\n\n$$\\min_{\\beta_0, \\beta} \\frac{1}{N} \\sum_{i = 1}^N w_i \\ell (y_i, \\beta_0 + \\beta^T x_i)\n    + \\lambda [0.5(1 - \\alpha)\\| \\beta \\|_2^2 + \\alpha \\| \\beta \\|_1]$$\n\nwhere $\\ell (y_i, \\beta_0 + \\beta^T x_i)$ is negative log-likelihood of\nan observation $i$. Here, we will go through Poisson link function case\nand show how we can optimize this cost function\n\nPoisson-like GLM\n----------------\nThe `pyglmnet` implementation comes with `poisson`, `binomial`\nand `normal` distributions, but for illustration, we will walk you\nthrough a particular adaptation of the canonical Poisson generalized\nlinear model (GLM).\n\nFor the Poisson GLM, $\\lambda_i$ is the rate parameter of an\ninhomogeneous linear-nonlinear Poisson (LNP) process with instantaneous\nmean given by:\n\n$$\\lambda_i = \\exp(\\beta_0 + \\beta^T x_i)$$\n\nwhere $x_i \\in \\mathcal{R}^{p \\times 1}, i = \\{1, 2, \\dots, n\\}$ are\nthe observed independent variables (predictors),\n$\\beta_0 \\in \\mathcal{R}^{1 \\times 1}$,\n$\\beta \\in \\mathcal{R}^{p \\times 1}$\nare linear coefficients. $\\lambda_i$ is also known as the conditional\nintensity function, conditioned on $(\\beta_0, \\beta)$ and\n$q(z) = \\exp(z)$ is the nonlinearity.\n\nFor numerical reasons, let's adopt a stabilizing non-linearity, known as the\nsoftplus or the smooth rectifier `Dugas et al., 2001\n<http://papers.nips.cc/paper/1920-incorporating-second-order-functional-knowledge-for-better-option-pricing.pdf>`_,\nand adopted by Jonathan Pillow's and Liam Paninski's groups for neural data\nanalysis.\nSee for instance: `Park et al., 2014\n<http://www.nature.com/neuro/journal/v17/n10/abs/nn.3800.html>`_.\n\n$$q(z) = \\log(1+\\exp(z))$$\n\nThe softplus prevents $\\lambda$ in the canonical inverse link function\nfrom exploding when the argument to the exponent is large. In this\nmodification, the formulation is no longer an exact LNP, nor an exact GLM,\nbut :math:\\pm\\mathcal{L}(\\beta_0, \\beta)` is still concave (convex) and we\ncan use gradient ascent (descent) to optimize it.\n\n$$\\lambda_i = q(\\beta_0 + \\beta^T x_i) = \\log(1 + \\exp(\\beta_0 +\n                           \\beta^T x_i))$$\n\n[There is more to be said about this issue; ref. Sara Solla's GLM lectures\nconcerning moment generating functions and strict definitions of GLMs]\n"
      ]
    },
    {
      "execution_count": null,
      "outputs": [],
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "def qu(z):\n    \"\"\"The non-linearity.\"\"\"\n    qu = np.log1p(np.exp(z))\n\ndef lmb(self, beta0, beta, X):\n    \"\"\"Conditional intensity function.\"\"\"\n    z = beta0 + np.dot(X, beta)\n    l = self.qu(z)\n    return l"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Poisson Log-likelihood\n^^^^^^^^^^^^^^^^^^^^^^\nThe likelihood of observing the spike count $y_i$ under the Poisson\nlikelihood function with inhomogeneous rate $\\lambda_i$ is given by:\n\n$$\\prod_i P(y = y_i) = \\prod_i \\frac{e^{-\\lambda_i} \\lambda_i^{y_i}}{y_i!}$$\n\nThe log-likelihood is given by:\n\n$$\\mathcal{L} = \\sum_i \\bigg\\{y_i \\log(\\lambda_i) - \\lambda_i\n                           - log(y_i!)\\bigg\\}$$\n\nHowever, we are interested in maximizing the log-likelihood as a function of\n$\\beta_0` and :math:`\\beta$. Thus, we can drop the factorial term:\n\n$$\\mathcal{L}(\\beta_0, \\beta) = \\sum_i \\bigg\\{y_i \\log(\\lambda_i)\n                                           - \\lambda_i\\bigg\\}$$\n\n\nElastic net penalty\n^^^^^^^^^^^^^^^^^^^\nFor large models we need to penalize the log likelihood term in order to\nprevent overfitting. The elastic net penalty is given by:\n\n$$\\mathcal{P}_\\alpha(\\beta) = (1-\\alpha)\\frac{1}{2} \\|\\beta\\|^2_{\\mathcal{l}_2} + \\alpha\\|\\beta\\|_{\\mathcal{l}_1}$$\n\nThe elastic net interpolates between two extremes.\n$\\alpha = 0` is known as ridge regression and :math:`\\alpha = 1$\nis known as LASSO. Note that we do not penalize the baseline term\n$\\beta_0$.\n"
      ]
    },
    {
      "execution_count": null,
      "outputs": [],
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "def penalty(alpha, beta):\n    \"\"\"the penalty term\"\"\"\n    P = 0.5 * (1 - alpha) * np.linalg.norm(beta, 2) + \\\n        alpha * np.linalg.norm(beta, 1)\n    return P"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Objective function\n^^^^^^^^^^^^^^^^^^\n\nWe minimize the objective function:\n\n$$J(\\beta_0, \\beta) = -\\mathcal{L}(\\beta_0, \\beta) + \\lambda \\mathcal{P}_\\alpha(\\beta)$$\n\nwhere $\\mathcal{L}(\\beta_0, \\beta)$ is the Poisson log-likelihood and\n$\\mathcal{P}_\\alpha(\\beta)$ is the elastic net penalty term and\n$\\lambda` and :math:`\\alpha$ are regularization parameters.\n"
      ]
    },
    {
      "execution_count": null,
      "outputs": [],
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "def loss(beta0, beta, reg_lambda, X, y):\n    \"\"\"Define the objective function for elastic net.\"\"\"\n    L = logL(beta0, beta, X, y)\n    P = penalty(beta)\n    J = -L + reg_lambda * P\n    return J"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Gradients descent\n^^^^^^^^^^^^^^^^^\n\nTo calculate the gradients of the cost function with respect to $\\beta_0$ and\n$\\beta$, let's plug in the definitions for the log likelihood and penalty terms from above.\n\n$$\\begin{eqnarray}\n        J(\\beta_0, \\beta) &= \\sum_i \\bigg\\{ \\log(1 + \\exp(\\beta_0 + \\beta^T x_i))\\\\\n          & - y_i \\log(\\log(1 + \\exp(\\beta_0 + \\beta^T x_i)))\\bigg\\}\\\\\n          & + \\lambda(1 - \\alpha)\\frac{1}{2} \\|\\beta\\|^2_{\\mathcal{l_2}}\n          + \\lambda\\alpha\\|\\beta\\|_{\\mathcal{l_1}}\n    \\end{eqnarray}$$\n\n\nSince we will apply co-ordinate descent, let's rewrite this cost in terms of each\nscalar parameter $\\beta_j$\n\n$$\\begin{eqnarray}\n        J(\\beta_0, \\beta) &= \\sum_i \\bigg\\{ \\log(1 + \\exp(\\beta_0 + \\sum_j \\beta_j x_{ij}))\n        & - y_i \\log(\\log(1 + \\exp(\\beta_0 + \\sum_j \\beta_j x_{ij})))\\bigg\\}\\\\\n        & + \\lambda(1-\\alpha)\\frac{1}{2} \\sum_j \\beta_j^2 + \\lambda\\alpha\\sum_j \\mid\\beta_j\\mid\n    \\end{eqnarray}$$\n\nLet's take the derivatives of some big expressions using chain rule.\nDefine $z_i = \\beta_0 + \\sum_j \\beta_j x_{ij}$.\n\nFor the nonlinearity in the first term $y = q(z) = \\log(1+e^{z(\\theta)})$,\n\n$$\\begin{eqnarray}\n    \\frac{\\partial y}{\\partial \\theta} &= \\frac{\\partial q}{\\partial z}\\frac{\\partial z}{\\partial \\theta}\\\\\n    & = \\frac{e^z}{1+e^z}\\frac{\\partial z}{\\partial \\theta}\\\\\n    & = \\sigma(z)\\frac{\\partial z}{\\partial \\theta}\n    \\end{eqnarray}$$\n\nFor the nonlinearity in the second term $y = \\log(q(z)) = \\log(\\log(1+e^{z(\\theta)}))$,\n\n$$\\begin{eqnarray}\n    \\frac{\\partial y}{\\partial \\theta} & = \\frac{1}{q(z)}\\frac{\\partial q}{\\partial z}\\frac{\\partial z}{\\partial \\theta}\\\\\n    & = \\frac{\\sigma(z)}{q(z)}\\frac{\\partial z}{\\partial \\theta}\n    \\end{eqnarray}$$\n\nwhere $\\dot q(z)$ happens to be be the sigmoid function,\n\n$$\\sigma(z) = \\frac{e^z}{1+e^z}$$\n\nPutting it all together, we have,\n\n$$\\frac{\\partial J}{\\partial \\beta_0} = \\sum_i \\sigma(z_i) - \\sum_i y_i\\frac{\\sigma(z_i)}{q(z_i)}$$\n\n$$\\frac{\\partial J}{\\partial \\beta_j} = \\sum_i \\sigma(z_i) x_{ij} - \\sum_i y_i \\frac{\\sigma(z_i)}{q(z_i)}x_{ij}\n    + \\lambda(1-\\alpha)\\beta_j + \\lambda\\alpha \\text{sgn}(\\beta_j)$$\n\nLet's define these gradients\n"
      ]
    },
    {
      "execution_count": null,
      "outputs": [],
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "def grad_L2loss(beta0, beta, reg_lambda, X, y):\n    z = beta0 + np.dot(X, beta)\n    s = expit(z)\n    q = qu(z)\n    grad_beta0 = np.sum(s) - np.sum(y * s / q)\n    grad_beta = np.transpose(np.dot(np.transpose(s), X) -\n                np.dot(np.transpose(y * s / q), X)) + \\\n    reg_lambda * (1 - alpha) * beta\n    return grad_beta0, grad_beta"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that this is all we need for a classic batch gradient descent implementation.\nHowever, let's also derive the Hessian terms that will be useful for second-order\noptimization methods.\n\nHessian terms\n^^^^^^^^^^^^^\n\nSecond-order derivatives can accelerate convergence to local minima by providing\noptimal step sizes. However, they are expensive to compute.\n\nThis is where co-ordinate descent shines. Since we update only one parameter\n$\\beta_j` per step, we can simply use the :math:`j^{th}$ diagonal term in\nthe Hessian matrix to perform an approximate Newton update as:\n\n$$\\beta_j^{t+1} = \\beta_j^{t} - \\bigg\\{\\frac{\\partial^2 J}{\\partial \\beta_j^2}\\bigg\\}^{-1} \\frac{\\partial J}{\\partial \\beta_j}$$\n\nLet's use calculus again to compute these diagonal terms. Recall that:\n\n$$\\begin{eqnarray}\n    \\dot q(z) & = \\sigma(z)\\\\\n    \\dot\\sigma(z) = \\sigma(z)(1-\\sigma(z))\n    \\end{eqnarray}$$\n\nUsing these, and applying the product rule\n\n$$\\frac{\\partial}{\\partial z}\\bigg\\{ \\frac{\\sigma(z)}{q(z)} \\bigg\\} = \\frac{\\sigma(z)(1-\\sigma(z))}{q(z)} - \\frac{\\sigma(z)}{q(z)^2}$$\n\nPlugging all these in, we get\n\n$$\\frac{\\partial^2 J}{\\partial \\beta_0^2} = \\sum_i \\sigma(z_i)(1 - \\sigma(z_i)) - \\sum_i y_i \\bigg\\{ \\frac{\\sigma(z_i) (1 - \\sigma(z_i))}{q(z_i)} - \\frac{\\sigma(z_i)}{q(z_i)^2} \\bigg\\}$$\n\n$$\\begin{eqnarray}\n    \\frac{\\partial^2 J}{\\partial \\beta_j^2} & = \\sum_i \\sigma(z_i)(1 - \\sigma(z_i)) x_{ij}^2 \\\\\n    & - \\sum_i y_i \\bigg\\{ \\frac{\\sigma(z_i) (1 - \\sigma(z_i))}{q(z_i)} \\\\\n    & - \\frac{\\sigma(z_i)}{q(z_i)^2} \\bigg\\}x_{ij}^2 + \\lambda(1-\\alpha)\n    \\end{eqnarray}$$\n"
      ]
    },
    {
      "execution_count": null,
      "outputs": [],
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "def hessian_loss(beta0, beta, alpha, reg_lambda, X, y):\n    z = beta0 + np.dot(X, beta)\n    q = qu(z)\n    s = expit(z)\n    grad_s = s * (1-s)\n    grad_s_by_q = grad_s/q - s/(q * q)\n    hess_beta0 = np.sum(grad_s) - np.sum(y * grad_s_by_q)\n    hess_beta = np.transpose(np.dot(np.transpose(grad_s), X * X)\n                - np.dot(np.transpose(y * grad_s_by_q), X * X))\\\n                + reg_lambda * (1-alpha)\n    return hess_beta0, hess_beta"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Cyclical co-ordinate descent\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n\n**Parameter update step**\n\nIn cylical coordinate descent with elastic net, we store an active set,\n$\\mathcal{K}`, of parameter indices that we update. Since the :math:`\\mathcal{l}_1$\nterms $|\\beta_j|$ are not differentiable at zero, we use the gradient without\nthe $\\lambda\\alpha \\text{sgn}(\\beta_j)` term to update :math:`\\beta_j$.\nLet's call these gradient terms $\\tilde{g}_k$.\n\nWe start by initializing $\\mathcal{K}$ to contain all parameter indices\nLet's say only the $k^{th}` parameter is updated at time step :math:`t$.\n\n$$\\begin{eqnarray}\n        \\beta_k^{t} & = \\beta_k^{t-1} - (h_k^{t-1})^{-1} \\tilde{g}_k^{t-1} \\\\\n        \\beta_j^{t} & = \\beta_j^{t-1}, \\forall j \\neq k\n    \\end{eqnarray}$$\n\n\nNext, we apply a soft thresholding step for $k \\neq 0$ after every update iteration, as follows.\n$\\beta_k^{t} = \\mathcal{S}_{\\lambda\\alpha}(\\beta_k^{t})$\n\nwhere\n\n$$S_\\lambda(x) =\n    \\begin{cases}\n    0 & \\text{if} & |x| \\leq \\lambda\\\\\n    \\text{sgn}(x)||x|-\\lambda| & \\text{if} & |x| > \\lambda\n    \\end{cases}$$\n\nIf $\\beta_k^{t}` has been zero-ed out, we remove :math:`k$ from the active set.\n\n$$\\mathcal{K} = \\mathcal{K} \\setminus \\left\\{k\\right\\}$$\n"
      ]
    },
    {
      "execution_count": null,
      "outputs": [],
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "def prox(X, l):\n    \"\"\"Proximal operator.\"\"\"\n    return np.sign(X) * (np.abs(X) - l) * (np.abs(X) > l)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Efficient z update**\n\nNext, we want to update $\\beta_{k+1}` at the next time step :math:`t+1$.\nFor this we need the gradient and Hessian terms, $\\tilde{g}_{k+1}$ and\n:math`h_{k+1}`. If we update them instead of recalculating them, we can save on\na lot of multiplications and additions. This is possible because we only update\none parameter at a time. Let's calculate how to make these updates.\n\n$$z_i^{t} = z_i^{t-1} - \\beta_k^{t-1}x_{ik} + \\beta_k^{t}x_{ik}$$\n\n$$z_i^{t} = z_i^{t-1} - (h_k^{t-1})^{-1} \\tilde{g}_k^{t-1}x_{ik}$$\n\n\n**Gradient update**\n\nIf $k = 0$,\n\n$$\\tilde{g}_{k+1}^t = \\sum_i \\sigma(z_i^t) - \\sum_i y_i \\frac{\\sigma(z_i^t)}{q(z_i^t)}$$\n\nIf $k > 0$,\n\n$$\\begin{eqnarray}\n        \\tilde{g}_{k+1}^t & = \\sum_i \\sigma(z_i^t) x_{i,k+1} - \\sum_i y_i \\frac{\\sigma(z_i^t)}{q(z_i^t)} x_{i,k+1}\n          & + \\lambda(1-\\alpha)\\beta_{k+1}^t\n    \\end{eqnarray}$$\n"
      ]
    },
    {
      "execution_count": null,
      "outputs": [],
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "def grad_loss_k(z, beta_k, alpha, rl, xk, y, k):\n    \"\"\"Gradient update for a single coordinate\n    \"\"\"\n    q = qu(z)\n    s = expit(z)\n    if(k == 0):\n        gk = np.sum(s) - np.sum(y*s/q)\n    else:\n        gk = np.sum(s*xk) - np.sum(y*s/q*xk) + rl*(1-alpha)*beta_k\n    return gk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Hessian update**\n\nIf $k = 0$,\n\n$$h_{k+1}^t & = \\sum_i \\sigma(z_i^t)(1 - \\sigma(z_i^t)) \\\\\n    & - \\sum_i y_i \\bigg\\{ \\frac{\\sigma(z_i^t) (1 - \\sigma(z_i^t))}{q(z_i^t)} - \\frac{\\sigma(z_i^t)}{q(z_i^t)^2} \\bigg\\}$$\n\n\nIf $k > 0$,\n\n$$\\begin{eqnarray}\n    h_{k+1}^t & = \\sum_i \\sigma(z_i^t)(1 - \\sigma(z_i^t)) x_{i,k+1}^2 \\\\\n    & - \\sum_i y_i \\bigg\\{ \\frac{\\sigma(z_i^t) (1 - \\sigma(z_i^t))}{q(z_i^t)}\n    & - \\frac{\\sigma(z_i^t)}{q(z_i^t)^2} \\bigg\\}x_{i,k+1}^2 + \\lambda(1-\\alpha)\n    \\end{eqnarray}$$\n"
      ]
    },
    {
      "execution_count": null,
      "outputs": [],
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "def hess_loss_k(z, alpha, rl, xk, y, k):\n    \"\"\"Hessian update for a single coordinate\n    \"\"\"\n    q = qu(z)\n    s = expit(z)\n    grad_s = s*(1-s)\n    grad_s_by_q = grad_s/q - s/(q*q)\n    if(k == 0):\n        hk = np.sum(grad_s) - np.sum(y*grad_s_by_q)\n    else:\n        hk = np.sum(grad_s*xk*xk) - np.sum(y*grad_s_by_q*xk*xk) + rl*(1-alpha)\n    return hk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Regularization paths and warm restarts\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nWe often find the optimal regularization parameter $\\lambda$ through cross-validation.\nThus, in practice, we fit the model several times over a range of $\\lambda$'s\n$\\{ \\lambda_{max} \\geq \\dots \\geq \\lambda_0\\}$.\n\nInstead of re-fitting the model each time, we can solve the problem for the\nmost-regularized model ($\\lambda_{max}$) and then initialize the subsequent\nmodel with this solution. The path that each parameter takes through the range of\nregularization parameters is known as the regularization path, and the trick of\ninitializing each model with the previous model's solution is known as a warm restart.\n\nIn practice, this significantly speeds up convergence.\n\n\nImplementation\n--------------\n\nThe optimization step is implemented in ``fit`` method in ``GLM``. We will add\npseudo code on how algorithm works soon.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "version": "3.5.1",
      "pygments_lexer": "ipython3",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      },
      "name": "python",
      "file_extension": ".py",
      "nbconvert_exporter": "python"
    }
  }
}