{
  "nbformat_minor": 0, 
  "nbformat": 4, 
  "cells": [
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "%matplotlib inline"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "\n# Elastic net regularized GLMs\n\nThis is an example demonstrating the internals of glmnet.\n\nJerome Friedman, Trevor Hastie and Rob Tibshirani. (2010).\nRegularization Paths for Generalized Linear Models via Coordinate Descent.\nJournal of Statistical Software, Vol. 33(1), 1-22 `[pdf]\n<https://core.ac.uk/download/files/153/6287975.pdf>`_.\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "# Author: Pavan Ramkumar\n# License: MIT\n\nimport numpy as np\nfrom scipy.special import expit\nimport scipy.sparse as sps\nimport matplotlib.pyplot as plt"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "GLM with elastic net penalty\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nPoisson-like GLM\n----------------\nThe `pyglmnet` implementation comes with `poisson`, `binomial`\nand `normal` distributions, but for illustration, we will walk you\nthrough a particular adaptation of the canonical Poisson generalized\nlinear model (GLM).\n\nFor the Poisson GLM, $\\lambda_i$ is the rate parameter of an\ninhomogeneous linear-nonlinear Poisson (LNP) process with instantaneous\nmean given by:\n\n$$\\lambda_i = \\exp(\\beta_0 + \\beta^T x_i)$$\n\nwhere $x_i \\in \\mathcal{R}^{p \\times 1}, i = \\{1, 2, \\dots, n\\}$ are\nthe observed independent variables (predictors),\n$\\beta_0 \\in \\mathcal{R}^{1 \\times 1}$,\n$\\beta \\in \\mathcal{R}^{p \\times 1}$\nare linear coefficients. $\\lambda_i$ is also known as the conditional\nintensity function, conditioned on $(\\beta_0, \\beta)$ and\n$q(z) = \\exp(z)$ is the nonlinearity.\n\nFor numerical reasons, let's adopt a stabilizing non-linearity, known as the\nsoftplus or the smooth rectifier `Dugas et al., 2001\n<http://papers.nips.cc/paper/1920-incorporating-second-order-functional-knowledge-for-better-option-pricing.pdf>`_,\nand adopted by Jonathan Pillow's and Liam Paninski's groups for neural data\nanalysis.\nSee for instance: `Park et al., 2014\n<http://www.nature.com/neuro/journal/v17/n10/abs/nn.3800.html>`_.\n\n$$q(z) = \\log(1+\\exp(z))$$\n\nThe softplus prevents $\\lambda$ in the canonical inverse link function\nfrom exploding when the argument to the exponent is large. In this\nmodification, the formulation is no longer an exact LNP, nor an exact GLM,\nbut :math:\\pm\\mathcal{L}(\\beta_0, \\beta)` is still concave (convex) and we\ncan use gradient ascent (descent) to optimize it.\n\n$$\\lambda_i = q(\\beta_0 + \\beta^T x_i) = \\log(1 + \\exp(\\beta_0 +\n                           \\beta^T x_i))$$\n\n[There is more to be said about this issue; ref. Sara Solla's GLM lectures\nconcerning moment generating functions and strict definitions of GLMs]\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "# Let's define the conditional intensity function\ndef qu(z):\n    eps = np.spacing(1)\n    return np.log(1 + eps + np.exp(z))\n\ndef lmb(beta0, beta, x):\n    eps = np.spacing(1)\n    z = beta0 + np.dot(x, beta)\n    return np.log(1 + eps + np.exp(z))"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "\nLog-likelihood\n^^^^^^^^^^^^^^\nThe likelihood of observing the spike count $y_i$ under the Poisson\nlikelihood function with inhomogeneous rate $\\lambda_i$ is given by:\n\n$$\\prod_i P(y = y_i) = \\prod_i \\frac{e^{-\\lambda_i} \\lambda_i^{y_i}}{y_i!}$$\n\nThe log-likelihood is given by:\n\n$$\\mathcal{L} = \\sum_i \\bigg\\{y_i \\log(\\lambda_i) - \\lambda_i\n                           - log(y_i!)\\bigg\\}$$\n\nHowever, we are interested in maximizing the log-likelihood as a function of\n$\\beta_0` and :math:`\\beta$. Thus, we can drop the factorial term:\n\n$$\\mathcal{L}(\\beta_0, \\beta) = \\sum_i \\bigg\\{y_i \\log(\\lambda_i)\n                                           - \\lambda_i\\bigg\\}$$\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Elastic net penalty\n^^^^^^^^^^^^^^^^^^^\nFor large models we need to penalize the log likelihood term in order to\nprevent overfitting. The elastic net penalty is given by:\n\n$$\\mathcal{P}_\\alpha(\\beta) = (1-\\alpha)\\frac{1}{2} \\|\\beta\\|^2_{\\mathcal{l}_2} + \\alpha\\|\\beta\\|_{\\mathcal{l}_1}$$\n\nThe elastic net interpolates between two extremes.\n$\\alpha = 0` is known as ridge regression and :math:`\\alpha = 1$\nis known as LASSO. Note that we do not penalize the baseline term\n$\\beta_0$.\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Let's define the penalty term\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "def penalty(alpha, beta):\n    P = 0.5 * (1 - alpha) * np.linalg.norm(beta, 2) + \\\n        alpha * np.linalg.norm(beta, 1)\n    return P"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }
  ], 
  "metadata": {
    "kernelspec": {
      "display_name": "Python 2", 
      "name": "python2", 
      "language": "python"
    }, 
    "language_info": {
      "mimetype": "text/x-python", 
      "nbconvert_exporter": "python", 
      "name": "python", 
      "file_extension": ".py", 
      "version": "2.7.11", 
      "pygments_lexer": "ipython2", 
      "codemirror_mode": {
        "version": 2, 
        "name": "ipython"
      }
    }
  }
}