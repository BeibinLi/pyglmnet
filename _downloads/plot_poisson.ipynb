{
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "execution_count": null,
      "outputs": [],
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# pyglmnet for Poisson distribution\n\n\nThis is an example demonstrating how pyglmnet works.\n\n"
      ]
    },
    {
      "execution_count": null,
      "outputs": [],
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "# Author: Pavan Ramkumar\n# License: MIT\n\nimport numpy as np\nimport scipy.sparse as sps\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here are inputs that you can provide when you instantiate the `GLM` class.\nIf not provided, it will be set to the respective defaults\n\n- `distr`: str, `'poisson'` or `'normal'` or `'binomial'` or `'multinomial'`\n    default: `'poisson'`\n- `alpha`: float, the weighting between L1 and L2 norm, default: 0.5\n- `reg_lambda`: array, array of regularized parameters,\n    default: `np.logspace(np.log(0.5), np.log(0.01), 10, base=np.exp(1))`\n- `learning_rate`: float, learning rate for gradient descent,\n    default: 1e-4\n- `max_iter`: int, maximum iteration for the model, default: 100\n"
      ]
    },
    {
      "execution_count": null,
      "outputs": [],
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "# import GLM model\nfrom pyglmnet import GLM\n\n# create regularize parameters for model\nreg_lambda = np.logspace(np.log(0.5), np.log(0.01), 10, base=np.exp(1))\nmodel = GLM(distr='poisson', verbose=False, alpha=0.05,\n            max_iter=1000, learning_rate=1e-4,\n            reg_lambda=reg_lambda)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Simulate a dataset\n------------------\nThe ``GLM`` class has a very useful method called ``simulate()``.\n\nSince a canonical link function is already specified by the distribution\nparameters, or provided by the user, ``simulate()`` requires\nonly the independent variables ``X`` and the coefficients ``beta0``\nand ``beta``\n"
      ]
    },
    {
      "execution_count": null,
      "outputs": [],
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "n_samples, n_features = 10000, 100\n\n# coefficients\nbeta0 = np.random.normal(0.0, 1.0, 1)\nbeta = sps.rand(n_features, 1, 0.1)\nbeta = np.array(beta.todense())\n\n# training data\nXr = np.random.normal(0.0, 1.0, [n_samples, n_features])\nyr = model.simulate(beta0, beta, Xr)\n\n# testing data\nXt = np.random.normal(0.0, 1.0, [n_samples, n_features])\nyt = model.simulate(beta0, beta, Xt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Fit the model\n^^^^^^^^^^^^^\nFitting the model is accomplished by a single GLM method called `fit()`.\nYou can provide data and output pair `(X, y)` i.e.\n"
      ]
    },
    {
      "execution_count": null,
      "outputs": [],
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "scaler = StandardScaler().fit(Xr)\nmodel.fit(scaler.transform(Xr), yr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visualize the fit coefficients\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nThe estimated coefficients are stored in an instance variable called ``.fit_``\nwhich is a list of dictionaries. Each dictionary corresponds to a\nparticular ``reg_lambda``\n"
      ]
    },
    {
      "execution_count": null,
      "outputs": [],
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "fit_param = model[0].fit_\nplt.plot(beta[:], 'bo')\nplt.hold(True)\nplt.plot(fit_param['beta'][:], 'ro')\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Slicing the model object\n^^^^^^^^^^^^^^^^^^^^^^^^\nAlthough the model is fit to all values of reg_lambda specified by a regularization\npath, often we are only interested in further analysis for a particular value of\n``reg_lambda``. We can easily do this by slicing the object.\n\nFor instance model[0] returns an object identical to model but with ``.fit_``\nas a dictionary corresponding to the estimated coefficients for ``reg_lambda[0]``.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Make predictions based on fit model\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nThe ``predict()`` method takes two parameters: a numpy 2d array of independent\nvariables and a dictionary of fit parameters. It returns a vector of\npredicted targets.\n"
      ]
    },
    {
      "execution_count": null,
      "outputs": [],
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "# Predict targets from test set\nyrhat = model[0].predict(scaler.transform(Xr))\nythat = model[0].predict(scaler.transform(Xt))\n\nplt.plot(yt[:100])\nplt.hold(True)\nplt.plot(ythat[:100], 'r')\nplt.xlabel('samples')\nplt.ylabel('true and predicted outputs')\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Goodness of fit\n^^^^^^^^^^^^^^^\nThe GLM class provides two methods for evaluating goodness of fit: ``deviance()``\nand ``pseudo_R2()``. Both of them require the true targets and the predicted targets\nas inputs. ``pseudo_R2()`` additionally requires a null model, which is typically\nthe mean of the target variables in the training set.\n"
      ]
    },
    {
      "execution_count": null,
      "outputs": [],
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "# Compute model deviance\nDr = model[0].deviance(yr, yrhat)\nDt = model[0].deviance(yt, ythat)\nprint(Dr, Dt)\n\n# Compute pseudo-R2s\nR2r = model[0].pseudo_R2(yr, yrhat, np.mean(yr))\nR2t = model[0].pseudo_R2(yt, ythat, np.mean(yr))\nprint(R2r, R2t)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Multinomial example\n^^^^^^^^^^^^^^^^^^^\nwe can also use ``pyglmnet`` with multinomial case\nwhere you can provide array of class\n"
      ]
    },
    {
      "execution_count": null,
      "outputs": [],
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "from sklearn.datasets import make_classification\nX, y = make_classification(n_samples=10000, n_classes=5,\n                           n_informative=100, n_features=100, n_redundant=0)\n\nmodel_mn = GLM(distr='multinomial', alpha=0.01,\n               reg_lambda=np.array([0.02, 0.01]), verbose=False)\nmodel_mn.threshold = 1e-5\nmodel_mn.fit(X, y)\ny_pred = model_mn[-1].predict(X).argmax(axis=1)\nprint('Output performance = %f percent' % (y_pred == y).mean())"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "version": "3.5.1",
      "pygments_lexer": "ipython3",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      },
      "name": "python",
      "file_extension": ".py",
      "nbconvert_exporter": "python"
    }
  }
}