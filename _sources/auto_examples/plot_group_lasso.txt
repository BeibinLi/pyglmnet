

.. _sphx_glr_auto_examples_plot_group_lasso.py:


=========================
Group Lasso Example
=========================

This is an example demonstrating Pyglmnet with
multinomial the group lasso regularization, typical in regression
problems where it is reasonable to impose penalties to model parameters
in a group-wise fashion based on domain knowledge.




.. code-block:: python


    from pyglmnet import GLM
    from pyglmnet.datasets import fetch_group_lasso_datasets
    import numpy as np
    import random








Group Lasso Example
similar to method found in:
ftp://ftp.stat.math.ethz.ch/Manuscripts/buhlmann/lukas-sara-peter.pdf

The task here is to determine which base pairs and positions within a 7-mer
sequence are most important to predicting if the sequence contains a splice
site or not.

#########################################################


.. code-block:: python


    print("Retrieving data...")

    df , group_idxs= fetch_group_lasso_datasets()


    print("Data retrieved")
    print("Dataframe: ")
    print(df.head())

    #set up the group lasso GLM model

    gl_glm = GLM(distr="binomial",
                 group=group_idxs,
                 tol=1e-2,
                 score_metric="pseudo_R2",
                 alpha=1.0,
                 reg_lambda=np.logspace(np.log(100), np.log(0.01), 5, base=np.exp(1)))


    #set up the non group GLM model

    glm = GLM(distr="binomial",
              tol=1e-2,
              score_metric="pseudo_R2",
              alpha=1.0,
              reg_lambda=np.logspace(np.log(100), np.log(0.01), 5, base=np.exp(1)))

    print("gl_glm: ", gl_glm)
    print("glm: ", glm)

    # Set up the training and testing sets.
    X = df[df.columns.difference(["Label"]).values]

    test_idxs = random.sample(list(range(X.shape[0])), 1000)
    train_idxs = list( set(list(range(X.shape[0]))).difference(set(test_idxs)) )

    X_train = X.iloc[train_idxs, :]
    X_test = X.iloc[test_idxs, :]

    y = df.loc[:, "Label"]
    y_train = y.iloc[train_idxs]
    y_test = y.iloc[test_idxs]


    print("Fitting models")
    gl_glm.fit(X_train.values, y_train.values)
    glm.fit(X_train.values, y_train.values)
    print("Model fitting complete.")
    print("\n\n")


    print("Group lasso post fitting score: ", gl_glm.score(X_test.values, y_test.values))
    print("Non-group lasso post fitting score: ", glm.score(X_test.values, y_test.values))




.. rst-class:: sphx-glr-script-out

 Out::

      Retrieving data...
    Data retrieved
    Dataframe: 
       0  1  2  3  4  5  6  7  8  9  ...    930  931  932  933  934  935  936  \
    0  1  1  0  1  0  1  0  0  0  0  ...      0    0    0    0    0    0    0   
    1  1  0  0  0  1  1  0  0  0  1  ...      0    0    0    0    0    0    0   
    2  1  1  0  1  0  1  0  0  0  0  ...      0    0    0    0    0    0    0   
    3  1  1  1  0  0  1  1  0  0  0  ...      0    0    0    0    0    0    0   
    4  1  0  0  0  1  0  1  1  0  0  ...      0    0    0    0    0    0    0   

       937  938  Label  
    0    0    0      1  
    1    0    0      1  
    2    0    0      1  
    3    0    0      1  
    4    0    0      1  

    [5 rows x 940 columns]
    gl_glm:  <
    Distribution | binomial
    alpha | 1.00
    max_iter | 1000.00
    lambda: 100.00 to 0.01
    >
    glm:  <
    Distribution | binomial
    alpha | 1.00
    max_iter | 1000.00
    lambda: 100.00 to 0.01
    >
    Fitting models
    Model fitting complete.



    Group lasso post fitting score:  [ 0.31214045  0.31701564  0.32167658  0.09503428  0.10014802]
    Non-group lasso post fitting score:  [  1.20247071e-04   1.04087812e-04   8.99707630e-05   7.76741899e-05
       7.63045243e-03]


**Total running time of the script:**
(0 minutes 31.275 seconds)



.. container:: sphx-glr-download

    **Download Python source code:** :download:`plot_group_lasso.py <plot_group_lasso.py>`


.. container:: sphx-glr-download

    **Download IPython notebook:** :download:`plot_group_lasso.ipynb <plot_group_lasso.ipynb>`
