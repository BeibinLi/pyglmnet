{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theano implementation of Poisson glmnet using cyclical coordinate descent\n",
    "\n",
    "Jerome Friedman, Trevor Hastie and Rob Tibshirani. (2008).\n",
    "Regularization Paths for Generalized Linear Models via Coordinate Descent.\n",
    "Journal of Statistical Software, Vol. 33(1), 1-22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import expit\n",
    "import scipy.sparse as sps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear nonlinear Poisson-like GLM with elastic net penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Poisson GLM\n",
    "For the Poisson generalized linear model (GLM), $\\lambda_i$ is the rate parameter of an inhomogeneous linear-nonlinear Poisson (LNP) process with instantaneous mean given by:\n",
    "\n",
    "$$\\lambda_i = \\exp(\\beta_0 + \\beta^T x_i)$$\n",
    "\n",
    "where $x_i \\in \\mathcal{R}^{p \\times 1}, i = \\{1, 2, \\dots, n\\}$ are the observed independent variables (predictors), $\\beta_0 \\in \\mathcal{R}^{1 \\times 1}$, $\\beta \\in \\mathcal{R}^{p \\times 1}$ are linear coefficients. $\\lambda_i$ is also known as the conditional intensity function, conditioned on $(\\beta_0, \\beta)$ and $q(z) = \\exp(z)$ is the nonlinearity.\n",
    "\n",
    "For numerical reasons, let's adopt a stabilizing non-linearity [adopted by Liam Paninski's group] $q(z) = \\log(1+\\exp(z))$ that prevents the $\\lambda_i$ terms from exploding when the argument to the exponent is large. In this modification, the formulation is no longer an exact LNP, nor an exact GLM, but $\\pm\\mathcal{L}(\\beta_0, \\beta)$ is still concave (convex) and we can use gradient ascent (descent) to optimize it.\n",
    "\n",
    "$$\\lambda_i = q(\\beta_0 + \\beta^T x_i) = \\log(1 + \\exp(\\beta_0 + \\beta^T x_i))$$\n",
    "\n",
    "[Refer to Sara Solla's GLM lectures concerning moment generating functions and strict definitions of GLMs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's define the conditional intensity function\n",
    "def qu(z):\n",
    "    return np.log(1+np.finfo(float).eps+np.exp(z))\n",
    "\n",
    "def lmb(beta0, beta, x):\n",
    "    z = beta0 + np.dot(x, beta)\n",
    "    return np.log(1+np.finfo(float).eps+np.exp(z))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log-likelihood\n",
    "The likelihood of observing the spike count $y_i$ under the Poisson likelihood function with inhomogeneous rate $\\lambda_i$ is given by:\n",
    "\n",
    "\\begin{equation}\n",
    "\\prod_i P(y = y_i) = \\prod_i \\frac{e^{-\\lambda_i} \\lambda_i^{y_i}}{y_i!}\n",
    "\\end{equation}\n",
    "\n",
    "The log-likelihood is given by:\n",
    "\n",
    "$$\\mathcal{L} = \\sum_i \\bigg\\{y_i \\log(\\lambda_i) - \\lambda_i - log(y_i!)\\bigg\\}$$\n",
    "\n",
    "However, we are interested in maximizing the log-likelihood as a function of $\\beta_0$ and $\\beta$. Thus, we can drop the factorial term:\n",
    "\n",
    "$$\\mathcal{L}(\\beta_0, \\beta) = \\sum_i \\bigg\\{y_i \\log(\\lambda_i) - \\lambda_i\\bigg\\}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's define the log likelihood\n",
    "def logL(beta0, beta, x, y):\n",
    "    lmb = lmb(beta0, beta, x)\n",
    "    logL = np.sum(y*np.log(lmb) - lmb)\n",
    "    return logL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elastic net penalty\n",
    "For large models we need to penalize the log likelihood term in order to prevent overfitting. \n",
    "The elastic net penalty is given by:\n",
    "\n",
    "$$\\mathcal{P}_\\alpha(\\beta) = (1-\\alpha)\\frac{1}{2} \\|\\beta\\|^2_{\\mathcal{l}_2} + \\alpha\\|\\beta\\|_{\\mathcal{l}_1}$$\n",
    "\n",
    "The elastic net interpolates between two extremes. $\\alpha = 0$ is known as ridge regression and $\\alpha = 1$ is known as Lasso. Note that we do not penalize the baseline term $\\beta_0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's define the penalty term\n",
    "def penalty(alpha, beta):\n",
    "    P = 0.5*(1-alpha)*np.linalg.norm(beta,2) + alpha*np.linalg.norm(beta,1)\n",
    "    return P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective function\n",
    "We minimize the objective function:\n",
    "\n",
    "$$J(\\beta_0, \\beta) = -\\mathcal{L}(\\beta_0, \\beta) + \\lambda \\mathcal{P}_\\alpha(\\beta)$$\n",
    "\n",
    "where $\\mathcal{L}(\\beta_0, \\beta)$ is the Poisson log-likelihood and $\\mathcal{P}_\\alpha(\\beta)$ is the elastic net penalty term and $\\lambda$ and $\\alpha$ are regularization parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's define the objective function with elastic net regularization\n",
    "def loss(beta0, beta, alpha, reg_lambda, x, y):\n",
    "    L = logL(beta0, beta, x, y)\n",
    "    P = penalty(alpha, beta)\n",
    "    J = -L + reg_lambda*P\n",
    "    return J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradients\n",
    "To calculate the gradients of the cost function with respect to $\\beta_0$ and $\\beta$, let's plug in the definitions for the log likelihood and penalty terms from above.\n",
    "\n",
    "$$\n",
    "J(\\beta_0, \\beta) = \\sum_i \\bigg\\{ \\log(1 + \\exp(\\beta_0 + \\beta^T x_i)) - y_i \\log(\\log(1 + \\exp(\\beta_0 + \\beta^T x_i)))\\bigg\\} + \\lambda(1-\\alpha)\\frac{1}{2} \\|\\beta\\|^2_{\\mathcal{l_2}} + \\lambda\\alpha\\|\\beta\\|_{\\mathcal{l_1}}\n",
    "$$\n",
    "\n",
    "Since we will apply co-ordinate descent, let's rewrite this cost in terms of each scalar parameter $\\beta_j$\n",
    "\n",
    "$$\n",
    "J(\\beta_0, \\beta) = \\sum_i \\bigg\\{ \\log(1 + \\exp(\\beta_0 + \\sum_j \\beta_j x_{ij})) - y_i \\log(\\log(1 + \\exp(\\beta_0 + \\sum_j \\beta_j x_{ij})))\\bigg\\} + \\lambda(1-\\alpha)\\frac{1}{2} \\sum_j \\beta_j^2 + \\lambda\\alpha\\sum_j \\mid\\beta_j\\mid\n",
    "$$\n",
    "\n",
    "Let's take the derivatives of some big expressions using chain rule. Define $z_i = \\beta_0 + \\sum_j \\beta_j x_{ij}$.\n",
    "\n",
    "For the nonlinearity in the first term $y = q(z) = \\log(1+e^{z(\\theta)})$,\n",
    "\n",
    "$$\\frac{\\partial y}{\\partial \\theta} = \\frac{\\partial q}{\\partial z}\\frac{\\partial z}{\\partial \\theta} = \\frac{e^z}{1+e^z}\\frac{\\partial z}{\\partial \\theta} = \\sigma(z)\\frac{\\partial z}{\\partial \\theta}$$\n",
    "\n",
    "For the nonlinearity in the second term $y = \\log(q(z)) = \\log(\\log(1+e^{z(\\theta)}))$,\n",
    "\n",
    "$$\\frac{\\partial y}{\\partial \\theta} = \\frac{1}{q(z)}\\frac{\\partial q}{\\partial z}\\frac{\\partial z}{\\partial \\theta} = \\frac{\\sigma(z)}{q(z)}\\frac{\\partial z}{\\partial \\theta}$$\n",
    "\n",
    "where $\\dot q(z)$ happens to be be the sigmoid function,\n",
    "$$\\sigma(z) = \\frac{e^z}{1+e^z}$$\n",
    "\n",
    "Putting it all together, we have,\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial \\beta_0} = \\sum_i \\sigma(z_i) - \\sum_i y_i\\frac{\\sigma(z_i)}{q(z_i)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial \\beta_j} = \\sum_i \\sigma(z_i) x_{ij} - \\sum_i y_i \\frac{\\sigma(z_i)}{q(z_i)}x_{ij}\n",
    "+ \\lambda(1-\\alpha)\\beta_j + \\lambda\\alpha \\text{sgn}(\\beta_j)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's define these gradients\n",
    "def grad_loss(beta0, beta, alpha, reg_lambda, x, y):\n",
    "    z = beta0 + np.dot(x, beta)\n",
    "    q = qu(z)\n",
    "    s = expit(z)\n",
    "    grad_beta0 = np.sum(s) - np.sum(y*s/q)\n",
    "    # This is a matrix implementation\n",
    "    grad_beta = np.transpose(np.dot(np.transpose(s), x) - np.dot(np.transpose(y*s/q), x)) \\\n",
    "        + reg_lambda*(1-alpha)*beta# + reg_lambda*alpha*np.sign(beta)\n",
    "    return grad_beta0, grad_beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hessian terms\n",
    "Second-order derivatives can accelerate convergence to local minima by providing optimal step sizes. However, they are expensive to compute. \n",
    "\n",
    "This is where co-ordinate descent shines. Since we update only one parameter $\\beta_j$ per step, we can simply use the $j^{th}$ diagonal term in the Hessian matrix to perform an approximate Newton update as:\n",
    "\n",
    "$$\\beta_j^{t+1} = \\beta_j^{t} - \\bigg\\{\\frac{\\partial^2 J}{\\partial \\beta_j^2}\\bigg\\}^{-1} \\frac{\\partial J}{\\partial \\beta_j}$$\n",
    "\n",
    "Let's use calculus again to compute these diagonal terms. Recall that:\n",
    "\n",
    "$$\\dot q(z) = \\sigma(z)$$ and \n",
    "$$\\dot\\sigma(z) = \\sigma(z)(1-\\sigma(z))$$\n",
    "\n",
    "Using these, and applying the product rule\n",
    "$$\n",
    "\\frac{\\partial}{\\partial z}\\bigg\\{ \\frac{\\sigma(z)}{q(z)} \\bigg\\} = \\frac{\\sigma(z)(1-\\sigma(z))}{q(z)} - \\frac{\\sigma(z)}{q(z)^2}\n",
    "$$\n",
    "\n",
    "Plugging all these in, we get\n",
    "$$\n",
    "\\frac{\\partial^2 J}{\\partial \\beta_0^2} = \\sum_i \\sigma(z_i)(1 - \\sigma(z_i)) - \\sum_i y_i \\bigg\\{ \\frac{\\sigma(z_i) (1 - \\sigma(z_i))}{q(z_i)} - \\frac{\\sigma(z_i)}{q(z_i)^2} \\bigg\\}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial^2 J}{\\partial \\beta_j^2} = \\sum_i \\sigma(z_i)(1 - \\sigma(z_i)) x_{ij}^2 \n",
    "- \\sum_i y_i \\bigg\\{ \\frac{\\sigma(z_i) (1 - \\sigma(z_i))}{q(z_i)} - \\frac{\\sigma(z_i)}{q(z_i)^2} \\bigg\\}x_{ij}^2 + \\lambda(1-\\alpha)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's define these Hessian terms\n",
    "def hessian_loss(beta0, beta, alpha, reg_lambda, x, y):\n",
    "    z = beta0 + np.dot(x, beta)\n",
    "    q = qu(z)\n",
    "    s = expit(z)\n",
    "    grad_s = s*(1-s)\n",
    "    grad_s_by_q = grad_s/q - s/(q*q)\n",
    "    \n",
    "    hess_beta0 = np.sum(grad_s) - np.sum(y*grad_s_by_q)\n",
    "    \n",
    "    # This is a matrix implementation\n",
    "    hess_beta = np.transpose(np.dot(np.transpose(grad_s), x*x) - np.dot(np.transpose(y*grad_s_by_q), x*x))\\\n",
    "                             + reg_lambda*(1-alpha)\n",
    "    return hess_beta0, hess_beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cyclical co-ordinate descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter update step\n",
    "In cylical coordinate descent with elastic net, we store an active set, $\\mathcal{K}$, of parameter indices that we update. Since the $\\mathcal{l}_1$ terms $|\\beta_j|$ are not differentiable at zero, we use the gradient without the $\\lambda\\alpha \\text{sgn}(\\beta_j)$ term to update $\\beta_j$. Let's call these gradient terms $\\tilde{g}_k$.\n",
    "\n",
    "We start by initializing $\\mathcal{K}$ to contain all parameter indices\n",
    "Let's say only the $k^{th}$ parameter is updated at time step $t$.\n",
    "\n",
    "$$\\beta_k^{t} = \\beta_k^{t-1} - (h_k^{t-1})^{-1} \\tilde{g}_k^{t-1}$$\n",
    "$$\\beta_j^{t} = \\beta_j^{t-1}, \\forall j \\neq k $$\n",
    "\n",
    "Next, we apply a soft thresholding step for $k \\neq 0$ after every update iteration, as follows.\n",
    "$$\\beta_k^{t} = \\mathcal{S}_{\\lambda\\alpha}(\\beta_k^{t})$$\n",
    "\n",
    "where \n",
    "$$S_\\lambda(x) = \n",
    "\\begin{cases}\n",
    "0 & \\text{if} & |x| \\leq \\lambda\\\\\n",
    "\\text{sgn}(x)|x-\\lambda| & \\text{if} & |x| > \\lambda\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "If $\\beta_k^{t}$ has been zero-ed out, we remove $k$ from the active set.\n",
    "$$\n",
    "\\mathcal{K} = \\mathcal{K} \\setminus \\left\\{k\\right\\}\n",
    "$$\n",
    "\n",
    "Next, we want to update $\\beta_{k+1}$ at the next time step $t+1$. For this we need the gradient and Hessian terms, $\\tilde{g}_{k+1}$ and $h_{k+1}$. If we update them instead of recalculating them, we can save on a lot of multiplications and additions. This is possible because we only update one parameter at a time. Let's calculate how to make these updates.\n",
    "\n",
    "$$z_i^{t} = z_i^{t-1} - \\beta_k^{t-1}x_{ik} + \\beta_k^{t}x_{ik}$$\n",
    "\n",
    "$$z_i^{t} = z_i^{t-1} - (h_k^{t-1})^{-1} \\tilde{g}_k^{t-1}x_{ik}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient update\n",
    "\n",
    "If $k = 0$,\n",
    "\n",
    "$$\\tilde{g}_{k+1}^t = -\\sum_i \\sigma(z_i^t) + \\sum_i y_i \\frac{\\sigma(z_i^t)}{q(z_i^t)}$$\n",
    "\n",
    "If $k > 0$,\n",
    "\n",
    "$$\\tilde{g}_{k+1}^t = -\\sum_i \\sigma(z_i^t) x_{i,k+1} + \\sum_i y_i \\frac{\\sigma(z_i^t)}{q(z_i^t)}x_{i,k+1} + \\lambda(1-\\alpha)\\beta_{k+1}^t$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hessian update\n",
    "\n",
    "If $k = 0$,\n",
    "\n",
    "$$\n",
    "h_{k+1}^t = -\\sum_i \\sigma(z_i^t)(1 - \\sigma(z_i^t)) \n",
    "+ \\sum_i y_i \\bigg\\{ \\frac{\\sigma(z_i^t) (1 - \\sigma(z_i^t))}{q(z_i^t)} - \\frac{\\sigma(z_i^t)}{q(z_i^t)^2} \\bigg\\}x_{i,k+1}^2\n",
    "$$\n",
    "\n",
    "If $k > 0$,\n",
    "\n",
    "$$\n",
    "h_{k+1}^t = -\\sum_i \\sigma(z_i^t)(1 - \\sigma(z_i^t)) x_{i,k+1}^2 \n",
    "+ \\sum_i y_i \\bigg\\{ \\frac{\\sigma(z_i^t) (1 - \\sigma(z_i^t))}{q(z_i^t)} - \\frac{\\sigma(z_i^t)}{q(z_i^t)^2} \\bigg\\}x_{i,k+1}^2 + \\lambda(1-\\alpha)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization paths and warm restarts\n",
    "\n",
    "Explain stuff about path and warm restarts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm\n",
    "\n",
    "Give the algorithm in an algorithm table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulate some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Simulate data\n",
    "N = 10000\n",
    "p = 1000\n",
    "x = np.random.normal(0.0, 1.0, [N,p])\n",
    "beta0 = np.random.normal(0.0, 1.0, 1)\n",
    "beta = sps.rand(p,1,0.01)\n",
    "beta = np.array(beta.todense())\n",
    "y = np.random.poisson(lmb(beta0, beta, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reg_lambda = 0.5\n",
    "alpha = 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "beta0_hat = np.random.normal(0.0,1.0,1)\n",
    "beta_hat = np.random.normal(0.0,1.0,[p,1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hess_beta0, hess_beta = hessian_loss(beta0_hat, beta_hat, alpha, reg_lambda, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grad_beta0, grad_beta = grad_loss(beta0_hat, beta_hat, alpha, reg_lambda, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "z = beta0_hat + np.dot(x, beta_hat)\n",
    "q = qu(z)\n",
    "s = expit(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show>"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXMAAAEACAYAAABBDJb9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3cbbfa6710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(beta_hat[:10])\n",
    "plt.hold\n",
    "plt.plot(1e-17*hess_beta[:10])\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Native python function\n",
    "def fitmodel(x, y, reg_lambda, alpha, weights):\n",
    "\n",
    "    n = x.shape[0]\n",
    "    p = x.shape[1]\n",
    "\n",
    "    # Outer loop with descending lambda\n",
    "\n",
    "        # Warm initialize parameters\n",
    "        \n",
    "        # Initialize active set\n",
    "        \n",
    "        #---------------------------\n",
    "        # Iterate until convergence\n",
    "        #---------------------------\n",
    "\n",
    "        # Middle loop for cycles\n",
    "        \n",
    "            # Inner loop for coordinate descent\n",
    "            no_convergence = 1\n",
    "            while(no_convergence):\n",
    "            for k in K:\n",
    "\n",
    "            #Update z\n",
    "            \n",
    "            #Calculate gradient\n",
    "\n",
    "            #Calculate Hessian\n",
    "\n",
    "            # Update the parameters\n",
    "            \n",
    "            # Soft threshold for k != 0\n",
    "            \n",
    "            # Update active set\n",
    "            \n",
    "        # Calculate cost and convergence criteria\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def glmnet(x, y, params):\n",
    "    \n",
    "    return fit\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
