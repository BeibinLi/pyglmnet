{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from theano import function\n",
    "from scipy.stats import zscore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test translating from numpy --> Theano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MyClass:\n",
    "    def __init__(self, distr='poisson'):\n",
    "        self.distr = distr\n",
    "    \n",
    "    #--------------\n",
    "    # nonlinearity\n",
    "    #--------------\n",
    "    def qu(self, z):\n",
    "        if(self.distr=='poisson'):\n",
    "            eps = T.dscalar('eps')\n",
    "            eps = 0.1\n",
    "            q = T.log(1+eps+T.exp(z))\n",
    "        elif(self.distr=='normal'):\n",
    "            q = z\n",
    "        elif(self.distr=='binomial'):\n",
    "            q = T.exp(z)/(1+T.exp(z))\n",
    "        return q\n",
    "    \n",
    "    #-----------------------\n",
    "    # conditional intensity\n",
    "    #-----------------------\n",
    "    def lmb(self, beta0, beta, x):\n",
    "        z = beta0 + T.dot(x,beta)\n",
    "        l = self.qu(z)\n",
    "        return l\n",
    "    \n",
    "    #-----------------\n",
    "    # log likelihoods\n",
    "    #-----------------\n",
    "    def logL(self, beta0, beta, x, y):\n",
    "        l = self.lmb(beta0, beta, x)\n",
    "        if(self.distr=='poisson'):\n",
    "            logL = T.sum(y*T.log(l) - l)\n",
    "        elif(self.distr=='normal'):\n",
    "            logL = -0.5*T.sum((y-l)**2)\n",
    "        elif(self.distr=='binomial'):\n",
    "            z = beta0 + T.dot(x,beta)\n",
    "            logL = T.sum(y*z - T.log(1+T.exp(z)))\n",
    "        return logL\n",
    "\n",
    "    #---------\n",
    "    # penalty\n",
    "    #---------\n",
    "    def penalty(self, alpha, beta):\n",
    "        P = 0.5*(1-alpha)*T.sum(beta**2) + alpha*T.sum(T.abs_(beta))\n",
    "        return P\n",
    "\n",
    "    #---------------\n",
    "    # loss function\n",
    "    #---------------\n",
    "    def loss(self, beta0, beta, alpha, reg_lambda, x, y):    \n",
    "        L = self.logL(beta0, beta, x, y)\n",
    "        P = self.penalty(alpha, beta)\n",
    "        J = -L + reg_lambda*P\n",
    "        return J\n",
    "\n",
    "    #--------------------------------------\n",
    "    # differentiable part of loss function\n",
    "    #--------------------------------------\n",
    "    def L2loss(self, beta0, beta, alpha, reg_lambda, x, y):\n",
    "        L = self.logL(beta0, beta, x, y)\n",
    "        P = 0.5*(1-alpha)*T.sum(beta**2)\n",
    "        J = -L + reg_lambda*P\n",
    "        return J\n",
    "\n",
    "    #-------------------\n",
    "    # proximal operator\n",
    "    #-------------------\n",
    "    def prox(self,x,l):\n",
    "        sx = T.sgn(x) * (T.abs_(x) - l) * (T.abs_(x) > l)\n",
    "        return sx\n",
    "\n",
    "    #-----------\n",
    "    # fit model\n",
    "    #-----------\n",
    "    def fit(self, x, y, reg_params, opt_params):\n",
    "    # Implements batch gradient descent (i.e. vanilla gradient descent by computing gradient over entire training set)\n",
    "        \n",
    "        # Dataset shape\n",
    "        n = x.shape[0]\n",
    "        p = x.shape[1]\n",
    "        \n",
    "        # Initialize shared variable\n",
    "        beta0 = theano.shared(np.random.randn(), name='beta0')\n",
    "        beta = theano.shared(np.random.randn(p), name='beta')\n",
    "        \n",
    "        # Regularization parameters\n",
    "        reg_lambda = reg_params['reg_lambda']\n",
    "        alpha = reg_params['alpha']\n",
    "\n",
    "        # Optimization parameters\n",
    "        max_iter = opt_params['max_iter']\n",
    "        e = opt_params['learning_rate']\n",
    "\n",
    "        # Initialize parameters\n",
    "        beta0_hat = np.random.randn()\n",
    "        beta_hat = np.random.randn(p)\n",
    "        fit = []\n",
    "\n",
    "        # Outer loop with descending lambda\n",
    "        for l,rl in enumerate(reg_lambda):\n",
    "            fit.append({'beta0': 0., 'beta': np.zeros(p), 'L': 10., 'DL': 10.})\n",
    "            len(fit)\n",
    "            print('Lambda: {}\\n').format(rl)\n",
    "\n",
    "            # Warm initialize parameters\n",
    "            if(l == 0):\n",
    "                fit[-1]['beta0'] = beta0_hat\n",
    "                fit[-1]['beta'] = beta_hat\n",
    "            else:\n",
    "                fit[-1]['beta0'] = fit[-2]['beta0']\n",
    "                fit[-1]['beta'] = fit[-2]['beta']\n",
    "\n",
    "            #---------------------------\n",
    "            # Iterate until convergence\n",
    "            #---------------------------\n",
    "            no_convergence = 1\n",
    "            convergence_threshold = 1e-3\n",
    "            t = 0\n",
    "\n",
    "            # Initialize parameters\n",
    "            beta0.set_value(fit[-1]['beta0'])\n",
    "            beta.set_value(fit[-1]['beta'])\n",
    "\n",
    "            # Initialize loss\n",
    "            L = []\n",
    "            DL = []\n",
    "\n",
    "            #Give formula for gradient\n",
    "            L2loss = self.L2loss(beta0, beta, alpha, rl, x, y)\n",
    "            grad_beta0, grad_beta = T.grad(L2loss, [beta0, beta])\n",
    "                \n",
    "            while(no_convergence and t < max_iter):\n",
    "                # Update time step\n",
    "                t = t+1\n",
    "                \n",
    "                # Update parameters\n",
    "                beta0 = beta0 -e*grad_beta0\n",
    "                beta = self.prox(beta -e*grad_beta, rl*alpha)\n",
    "\n",
    "                # Calculate loss\n",
    "                L.append(self.loss(beta0, beta, alpha, rl, x, y).eval())\n",
    "                print('    iter:{}, loss:{}'.format(t, L[-1]))\n",
    "                # Delta loss and convergence criterion\n",
    "                if t > 1:\n",
    "                    DL.append(L[-1] - L[-2])\n",
    "                    if(np.abs(DL[-1]/L[-1]) < convergence_threshold):\n",
    "                        no_convergence = 0\n",
    "                        print('Converged')\n",
    "                        print('    Loss function: {}').format(L[-1])\n",
    "                        print('    dL/L: {}\\n').format(DL[-1]/L[-1])\n",
    "\n",
    "                #if t==99:\n",
    "                #        no_convergence = 0\n",
    "                #        print('Converged')\n",
    "                        \n",
    "            #Store the parameters after convergence\n",
    "            print beta0.eval()\n",
    "            fit[-1]['beta0'] = beta0.eval()\n",
    "            fit[-1]['beta'] = beta.eval()\n",
    "\n",
    "        return fit\n",
    "\n",
    "    #-----------------------------\n",
    "    # Define the predict function\n",
    "    #-----------------------------\n",
    "    def predict(self, x, fitparams):\n",
    "        yhat = self.lmb(fitparams['beta0'], fitparams['beta'], zscore(x))\n",
    "        return yhat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mm = MyClass('poisson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO (theano.gof.compilelock): Refreshing lock /home/pavan/.theano/compiledir_Linux-3.16--generic-x86_64-with-debian-jessie-sid-x86_64-2.7.11-64/lock_dir/lock\n"
     ]
    }
   ],
   "source": [
    "N = 1000\n",
    "p = 100\n",
    "\n",
    "x = np.random.randn(N,p)\n",
    "beta = np.random.randn(p)\n",
    "beta0 = np.random.randn()\n",
    "y = np.random.poisson(mm.lmb(beta0, beta, x).eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda: 0.0154445210495\n",
      "\n",
      "<TensorType(float64, scalar)>\n",
      "    iter:1\n",
      "    iter:1, loss:3697.5048762\n",
      "<TensorType(float64, scalar)>\n",
      "    iter:2\n",
      "    iter:2, loss:3591.85011166\n",
      "<TensorType(float64, scalar)>\n",
      "    iter:3\n",
      "    iter:3, loss:3486.48924325\n",
      "<TensorType(float64, scalar)>\n",
      "    iter:4\n",
      "    iter:4, loss:3381.74311572\n",
      "<TensorType(float64, scalar)>\n",
      "    iter:5\n",
      "    iter:5, loss:3277.64168672\n",
      "<TensorType(float64, scalar)>\n",
      "    iter:6\n",
      "    iter:6, loss:3174.28916017\n",
      "<TensorType(float64, scalar)>\n",
      "    iter:7\n",
      "    iter:7, loss:3071.72845449\n",
      "<TensorType(float64, scalar)>\n",
      "    iter:8\n",
      "    iter:8, loss:2969.96637885\n",
      "<TensorType(float64, scalar)>\n",
      "    iter:9\n",
      "    iter:9, loss:2869.00312926\n",
      "<TensorType(float64, scalar)>\n",
      "    iter:10\n",
      "    iter:10, loss:2768.85645198\n",
      "<TensorType(float64, scalar)>\n",
      "    iter:11\n",
      "    iter:11, loss:2669.57506652\n",
      "<TensorType(float64, scalar)>\n",
      "    iter:12\n",
      "    iter:12, loss:2571.2408361\n",
      "<TensorType(float64, scalar)>\n",
      "    iter:13\n",
      "    iter:13, loss:2474.97523886\n",
      "<TensorType(float64, scalar)>\n",
      "    iter:14\n",
      "    iter:14, loss:2380.27771348\n",
      "<TensorType(float64, scalar)>\n",
      "    iter:15\n",
      "    iter:15, loss:2287.00519311\n",
      "<TensorType(float64, scalar)>\n",
      "    iter:16\n"
     ]
    }
   ],
   "source": [
    "# Set regularization parameters\n",
    "reg_lambda = np.logspace(np.log(0.5), np.log(0.01), 10, base=np.exp(1))\n",
    "alpha = 0.1\n",
    "\n",
    "fit_params = dict()\n",
    "fit_params['reg_lambda'] = reg_lambda[-2:]\n",
    "fit_params['alpha'] = alpha\n",
    "\n",
    "# Set optimization parameters\n",
    "opt_params = dict()\n",
    "opt_params['learning_rate'] = 1e-4\n",
    "opt_params['max_iter'] = 1000\n",
    "\n",
    "# Fit model to training data\n",
    "fit = mm.fit(zscore(x),y,fit_params,opt_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time numpy vs. theano fixed point operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "z = T.dvector('z')\n",
    "q = T.log(1+0.1+T.exp(z))\n",
    "qu = function([z], q)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z0 = np.random.randn(200000)\n",
    "type(z0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 loops, best of 3: 13.5 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit qu(z0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def qu_np(z):\n",
    "    eps = 0.1\n",
    "    q = np.log(1+eps+np.exp(z))\n",
    "    return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 loops, best of 3: 12.4 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit qu_np(z0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MyClass:\n",
    "    # qu function in theano\n",
    "    z = T.dvector('z')\n",
    "    q = T.log(1+0.1+T.exp(z))\n",
    "    qu = function([z], q)\n",
    "    \n",
    "    # lmb function in theano\n",
    "    x = T.dmatrix('x')\n",
    "    beta = T.dvector('beta')\n",
    "    beta0 = T.dscalar('beta0')\n",
    "    z = beta0 + T.dot(x,beta)\n",
    "    l = qu(z)\n",
    "    #l = T.log(1+0.1+T.exp(z))\n",
    "    lmb = function([beta0, beta, x], l)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
